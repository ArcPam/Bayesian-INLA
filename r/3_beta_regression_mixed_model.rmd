---
title: "Modeling Inoculum Availability of *Plurivorosphaerella nawae* in Persimmon Leaf Litter with Bayesian Beta Regression"
author: "Joaquín Martínez-Minaya"
date: "`r Sys.Date()`"
linestretch: "1.5"

output:   
  bookdown::html_document2:
  #theme: cerulean
    df_print: paged
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

This is a simplified version of the paper [Martinez-Minaya et al., 2021](https://apsjournals.apsnet.org/doi/full/10.1094/PHYTO-08-20-0359-R). For a complete version of the problem, visit this [repository](https://bitbucket.org/joaquin-martinez-minaya/betapathogen/src)

```{r, warning = FALSE, message = FALSE}
  library(dplyr)
  library(ggplot2)
  library(INLA)
``` 
# Introduction to the problem

Circular leaf spot (CLS), caused by *Plurivorosphaerella nawae*, is a serious disease affecting persimmon that is characterized by:

 - Necrotic lesions on leaves,

 - Defoliation, and 

 - Fruit drop. 

 
Under Mediterranean conditions, *P. nawae* forms pseudothecia in the leaf litter in winter, and ascospores are released in spring, infecting susceptible leaves. Persimmon growers are advised to apply fungicides for CLS control during the period of inoculum availability, which was previously defined based on ascospore counts under the microscope. A model of inoculum availability of *P.  nawaew* as developed and evaluated as an alternative to ascospore counts. Leaf litter samples were collected weekly in L’Alcudia (Spain) from 2010 to 2015. Leaves were soaked and placed in a wind tunnel, and the released ascospores of *P. nawae were counted. 
 

# Importing the data from L'Alcúdia
The models were constructed using the data from L'Alcúdia. The variables available are:

1. *y*: response variable. It is the proportion of ascospores. Note that in this exercise, the response variable has been previously transformed using the function ***DR_data*** from the package ***DirichletReg***.

2. *year*: the year where the proportion of ascospores has been measured.

3. *dd*: acumulatted degree days. Biofix 01-01 and Tbase = 0ºC.





```{r,echo= TRUE, warning= FALSE, message= FALSE}
# Ascospores data
# data <- readRDS("../data/data_khaki/alcudiadata_01-01.RDS")
# data <- data %>%
#   select(prop, Fe, anyo, dd) %>%
#   filter(dd <= 40)
# 
# data$prop <- DirichletReg::DR_data(data$prop)[,2]
# colnames(data) <- c("y", "date", "year", "dd")
# writexl::write_xlsx(data, "../data/data_khaki/data_alcudia.xlsx")
# 
data <- readxl::read_excel("../data/data_khaki/data_alcudia.xlsx")
data$year <- factor(data$year)
data <- data %>%
  dplyr::select(y, year, dd)
head(data)
```


# Descriptive analysis
```{r}
  ggplot(data) +
  geom_point(aes(x = dd, y = y, col = year))+
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_color_brewer(palette="Dark2") +
  xlab("dd ( x 100)")
```


# The model: Hierarchical Bayesian Beta regression

To model a proportion, we use the Beta likelihood. Usually it depends on two parameters: $p$ and $q$, but, we use the reparameterization in terms of the mean and the precision.




# Fitting models {.tabset}

## Just intercept
```{r}
formula0 <- y ~ + 1 
mod0 <- inla(formula0, 
             family            = "beta",
             data              = data,
             control.family    = list(link  = "logit"),
             control.compute   = list(dic    = TRUE, 
                                      cpo    = TRUE, 
                                      waic   = TRUE),
             control.predictor = list(compute = TRUE), 
             control.inla      = list(strategy = 'simplified.laplace'))

```



## Fixed effect

```{r}
formula1 <- y ~ + 1 + dd 

mod1 <- inla(formula1, 
             family            = "beta",
             data              = data,
             control.family    = list(link  = "logit"),
             control.compute   = list(dic    = TRUE, 
                                      cpo    = TRUE, 
                                      waic   = TRUE),
             control.predictor = list(compute = TRUE), 
             control.inla      = list(strategy = 'simplified.laplace'))

```


## Random effect

In order to introduce a random effect year in the formula, we create an index:

```{r}
  data$year_ind <- as.numeric(data$year)
```   

```{r}
prior1_tau <- list(prec = list(prior = 'pc.prec', param = c(5, 0.01)))

formula2 <- y ~ + 1 + dd + 
  f(year_ind, model='iid', 
    hyper = prior1_tau) 

mod2 <- inla(formula2, 
             family            = "beta",
             data              = data,
             control.family    = list(link  = "logit"),
             control.compute   = list(config = TRUE,
                                      dic    = TRUE, 
                                      cpo    = TRUE, 
                                      waic   = TRUE),
             control.predictor = list(compute = TRUE), 
             control.inla      = list(strategy = 'simplified.laplace'))

```


# Comparing models 
```{r}
  selection <- data.frame(DIC  = c(mod0$dic$dic, mod1$dic$dic, mod2$dic$dic),
                        WAIC = c(mod0$waic$waic, mod1$waic$waic, mod2$waic$waic),
                        LCPO = c(-mean(log(mod0$cpo$cpo)),
                                 -mean(log(mod1$cpo$cpo)),
                                 -mean(log(mod2$cpo$cpo))))
rownames(selection)<- c("Null model", "DD", "DD + year")
selection


```

# Best model {.tabset} 

## Posterior distribution of the fixed effects
```{r}
  mod2$summary.fixed %>% round(., 4)
```


```{r, fig.asp = 0.5}
post_margs <- mod2$marginals.fixed %>%
  lapply(., inla.smarginal) %>%
  do.call(rbind.data.frame, .)

rownames(post_margs) %>% 
  stringr::str_split(., "\\.") %>%
  purrr::map(., 1) %>% unlist(.) -> post_margs$variable

ggplot(post_margs, aes(x = x, y = y)) +
  geom_line(aes(color = variable), size = 1.3) +
  facet_wrap(~variable, ncol = 2, scales = "free") +
  theme_bw() +
  theme(legend.position = "none")

```


## Posterior distribution of the hyperparameters
```{r}
  mod2$summary.hyperpar %>% round(., 4)
```


```{r, fig.asp = 0.5}
#Standard deviation of the random effect year  
sd_re <- inla.tmarginal(function(x)1/sqrt(x), mod2$marginals.hyperpar$`Precision for year`)

#Precision of the beta effect
prec_beta <- inla.smarginal(mod2$marginals.hyperpar$`precision parameter for the beta observations`)

#Joining both  
hyperpars <- rbind(data.frame(sd_re, hyp1 = "sigma-year"),
                     data.frame(prec_beta, hyp1 = "prec-beta"))
hyperpars$hyp1 <- factor(hyperpars$hyp1)

#Plotting
ggplot(hyperpars, aes(x = x, y = y)) +
    geom_line(aes(color = hyp1), size = 1.3) + 
    facet_wrap(~ hyp1, ncol = 2, scales = "free") +
    theme_bw() +
    theme(legend.position = "none")
    
```


# Is the effect dd relevant in the model?

```{r}
  p1 <- 1 - inla.pmarginal(0, mod2$marginals.fixed$dd)
```
Yes! It is relevant, as the $P(\beta_1 > 0 \mid \boldsymbol{y}) =$ `r p1`.


# Posterior predictive distribution of the mean
Once the best model has been fitted, we make predictions. Usually, we do predictions on the mean without having in mind the response variability of the response. Here, we depict the code for these predictions.


First, we create a grid for the values of the DD. We create a response being `NA` and a new value for year\_ind We join this new dataset for predictions with the one used for fitting. Then, we rerun the `inla` again.

```{r, echo= TRUE, warning= FALSE, message= FALSE}
### ----- 4.2. Predictions --- ####

x_dd <- seq(5, 40, 0.5)

data_pred <- data.frame(y = NA,
                        year = 2016,
                        dd   = x_dd,
                        year_ind = (max(data$year_ind) + 1))

data_total <- rbind(data,
                    data_pred)


formula2
mod2.pred <- inla(formula2,
                  family            = "beta",
                  data              = data_total,
                  control.family    = list(link  = "logit"),
                  control.compute   = list(config = TRUE,
                                           dic    = TRUE, 
                                           cpo    = TRUE, 
                                           waic   = TRUE),
                  control.predictor = list(compute = TRUE,
                                           link    = 1), 
                  control.mode      = list(theta = mod2$mode$theta,
                                            restart = TRUE),
                  control.inla      = list(strategy = 'simplified.laplace'),
                  verbose = FALSE)

```

```{r}
  predictive <- mod2.pred$summary.fitted.values[-c(1:dim(data)[1]),]
  predictive <- cbind(data_pred, predictive)
  colnames(predictive) <- c("y", "year", "dd", "year_ind", "mean", "sd", "Quant025", "Quant50", "Quant975", "mode")
```

```{r}
  ggplot() +
    geom_line(data = predictive, aes_string(x = "dd", y = "Quant50")) +
    geom_ribbon(data = predictive, 
                aes_string(x = "dd",
                           ymin = "Quant025", 
                           ymax = "Quant975"), alpha = 0.1) +
  geom_point(data = data, aes(x = dd, y = y, col = year))+
  theme_bw() +
  theme(legend.position = "bottom") +
  scale_color_brewer(palette="Dark2") +
  xlab("dd ( x 100)")
  
```


<!-- ## Posterior predictive distribution for the response -->

<!-- After, we simulated 10000 samples from the posteriors (parameters and hyperparameters), and computed the posterior predictive distribution for the response variable. -->

<!-- ```{r, echo= TRUE, results= 'hide', warning= FALSE, message= FALSE, eval = FALSE} -->
<!-- samples_al_a <- inla.posterior.sample(n   = 100,  -->
<!--                                       res = mod2.pred) -->


<!-- samples_al_a[[1]]$latent -->
<!-- psam_al_a <- sapply(samples_al_a, function(x) { -->
<!--   res <- numeric() -->
<!--   prec <- x$hyperpar[1] -->
<!--   mu_new<-numeric() -->
<!--   k <- 0 -->
<!--   predictor<-numeric() -->
<!--   for (i in 1:length(x_dd)) -->
<!--   { -->
<!--     for (j in 1:length(x_ddvpd)) -->
<!--     { -->
<!--       k <- k + 1 -->
<!--       predictor[k]<- x$latent[dim(x$latent)[1]-2] +  -->
<!--         x$latent[dim(x$latent)[1] - 1] * x_dd[i] + -->
<!--         x$latent[dim(x$latent)[1]] * x_ddvpd[j] + -->
<!--         rnorm(1, 0, sd=sqrt(1/x$hyperpar[2]))  -->

<!--       mu_new[k] <- exp(predictor[k])/(1 + exp(predictor[k])) -->

<!--     } -->
<!--     #print(k) -->
<!--   } -->

<!--   p<-mu_new * prec -->
<!--   q<--mu_new*prec + prec -->
<!--   for (i in 1:length(p)) -->
<!--   { -->
<!--     res[i] <- rbeta(1, p[i], q[i]) -->

<!--   } -->
<!--   print("good") -->
<!--   res  -->
<!-- }) -->


<!-- ### --- Computing credible intervals for the samples ---  -->
<!-- q.sam_al_a <- apply(psam_al_a, 1, quantile, -->
<!--                     c(.025, 0.05, 0.5, 0.95, .975), na.rm =TRUE) -->


<!-- ``` -->




